import { NextRequest, NextResponse } from 'next/server'
import { createClients } from '@/utils/supabase/server'
import { parseLanguageForWhisper } from '@/utils/transcription-language'
import { convertToSimplified } from '@/utils/chinese-converter'
import { isAudioSilentSimple } from '@/utils/audio-silence-detector'
import OpenAI from 'openai'
import { File as UndiciFile } from 'undici'

// Vercel å‡½æ•°é…ç½®ï¼šè®¾ç½®æœ€å¤§æ‰§è¡Œæ—¶é—´ä¸º 10 åˆ†é’Ÿï¼ˆ600ç§’ï¼‰
// è¿™å¯¹äºå¤„ç†é•¿éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚10åˆ†é’Ÿå½•éŸ³ï¼‰æ˜¯å¿…éœ€çš„
export const maxDuration = 600 // 10åˆ†é’Ÿ = 600ç§’
export const dynamic = 'force-dynamic'

// æ ¼å¼åŒ–æ—¶é—´æˆ³
function formatTimestamp(seconds: number): string {
  const mins = Math.floor(seconds / 60)
  const secs = Math.floor(seconds % 60)
  return `[${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}]`
}

// POST /api/meetings/[id]/transcribe - è½¬å½•éŸ³é¢‘æ–‡ä»¶
export async function POST(
  request: NextRequest,
  { params }: { params: Promise<{ id: string }> }
) {
  try {
    const { id } = await params
    
    // æ£€æŸ¥è¯·æ±‚å†…å®¹ç±»å‹ï¼Œæ”¯æŒ FormData æˆ– JSON
    const contentType = request.headers.get('content-type') || ''
    let audioFile: File | null = null
    let language: string = 'auto'
    let audioDurationStr: string | null = null
    let audioUrl: string | null = null
    
    if (contentType.includes('application/json')) {
      // JSON æ ¼å¼ï¼šæ”¯æŒ audioUrlï¼ˆç”¨äºå¤§æ–‡ä»¶ä¸Šä¼ ï¼‰
      const body = await request.json()
      audioUrl = body.audioUrl || null
      language = body.language || 'auto'
      audioDurationStr = body.audioDuration || null
      
      if (!audioUrl) {
        return NextResponse.json({ error: 'No audioUrl provided' }, { status: 400 })
      }
      
      console.log('ğŸ“ æ¥æ”¶åˆ°éŸ³é¢‘ URL:', {
        audioUrl,
        language,
        audioDuration: audioDurationStr
      })
      
      // ä» URL ä¸‹è½½éŸ³é¢‘æ–‡ä»¶
      try {
        // æ£€æŸ¥æ˜¯å¦æ˜¯ base64 data URLï¼ˆå¦‚æœæ²¡æœ‰é…ç½® Blob Storageï¼‰
        if (audioUrl.startsWith('data:')) {
          console.warn('âš ï¸ Received base64 data URL, attempting to convert...')
          // è§£æ base64 data URL
          const matches = audioUrl.match(/^data:([^;]+);base64,(.+)$/)
          if (!matches) {
            throw new Error('Invalid base64 data URL format')
          }
          const mimeType = matches[1]
          const base64Data = matches[2]
          const buffer = Buffer.from(base64Data, 'base64')
          // ä» MIME ç±»å‹æ¨æ–­æ–‡ä»¶æ‰©å±•å
          const extension = mimeType.split('/')[1] || 'wav'
          const fileName = `audio.${extension}`
          // ä½¿ç”¨ Buffer ç›´æ¥åˆ›å»º UndiciFileï¼ˆé¿å… Blob ç±»å‹ä¸å…¼å®¹é—®é¢˜ï¼‰
          audioFile = new UndiciFile([buffer], fileName, { type: mimeType }) as unknown as File
          console.log('âœ… ä» base64 data URL è½¬æ¢éŸ³é¢‘æ–‡ä»¶æˆåŠŸ:', {
            name: audioFile.name,
            size: audioFile.size,
            type: audioFile.type
          })
        } else {
          // æ™®é€š URLï¼Œä½¿ç”¨ fetch ä¸‹è½½
          const response = await fetch(audioUrl)
          if (!response.ok) {
            throw new Error(`Failed to download audio from URL: ${response.statusText}`)
          }
          const arrayBuffer = await response.arrayBuffer()
          // ä» URL æ¨æ–­æ–‡ä»¶åå’Œ MIME ç±»å‹
          const urlPath = new URL(audioUrl).pathname
          const fileName = urlPath.split('/').pop() || 'audio.wav'
          const contentType = response.headers.get('content-type') || 'audio/wav'
          // ä½¿ç”¨ Uint8Array åˆ›å»º UndiciFileï¼ˆé¿å… Blob ç±»å‹ä¸å…¼å®¹é—®é¢˜ï¼‰
          audioFile = new UndiciFile([new Uint8Array(arrayBuffer)], fileName, { type: contentType }) as unknown as File
          console.log('âœ… ä» URL ä¸‹è½½éŸ³é¢‘æ–‡ä»¶æˆåŠŸ:', {
            name: audioFile.name,
            size: audioFile.size,
            type: audioFile.type
          })
        }
      } catch (downloadError: any) {
        console.error('âŒ ä» URL ä¸‹è½½/è½¬æ¢éŸ³é¢‘æ–‡ä»¶å¤±è´¥:', downloadError)
        return NextResponse.json({ 
          error: `Failed to process audio URL: ${downloadError.message}` 
        }, { status: 400 })
      }
    } else {
      // FormData æ ¼å¼ï¼šç›´æ¥ä¸Šä¼ çš„æ–‡ä»¶
      const formData = await request.formData()
      audioFile = formData.get('audio') as File
      language = formData.get('language') as string || 'auto'
      audioDurationStr = formData.get('audioDuration') as string || null
      
      if (!audioFile) {
        return NextResponse.json({ error: 'No audio file provided' }, { status: 400 })
      }
      
      console.log('ğŸ“ æ¥æ”¶åˆ°çš„éŸ³é¢‘æ–‡ä»¶:', {
        name: audioFile.name,
        size: audioFile.size,
        type: audioFile.type,
        language: language,
        audioDuration: audioDurationStr
      })
    }
    
    // éªŒè¯éŸ³é¢‘æ–‡ä»¶
    if (!audioFile || audioFile.size === 0) {
      return NextResponse.json({ error: 'Audio file is empty' }, { status: 400 })
    }
    
    // è·å–å®é™…å½•éŸ³æ—¶é•¿ï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ä¼°ç®—
    const audioDuration = audioDurationStr ? parseFloat(audioDurationStr) : null

    // æ£€æŸ¥ API å¯†é’¥
    if (!process.env.WHISPER_API_KEY) {
      return NextResponse.json(
        { error: 'WHISPER_API_KEY not configured' },
        { status: 500 }
      )
    }

    console.log('ğŸ”Š ä½¿ç”¨ LemonFox.ai Whisper API è¿›è¡Œè½¬å½•:', {
      audioFileSize: audioFile.size,
      audioFileType: audioFile.type,
      language: language
    })

    // é™éŸ³æ£€æµ‹ï¼šåœ¨è°ƒç”¨ Whisper API ä¹‹å‰å…ˆæ£€æµ‹éŸ³é¢‘æ˜¯å¦ä¸ºé™éŸ³
    // å¯¹äºå‹ç¼©æ ¼å¼ï¼ˆM4A, MP3ç­‰ï¼‰ï¼ŒRMS å€¼é€šå¸¸è¾ƒä½ï¼Œä½¿ç”¨æ›´ä½çš„é˜ˆå€¼
    try {
      // æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„é˜ˆå€¼
      const fileType = audioFile.type?.toLowerCase() || ''
      const fileName = audioFile.name?.toLowerCase() || ''
      const isCompressedFormat = fileType.includes('m4a') || fileType.includes('mp3') || 
                                 fileType.includes('aac') || fileName.endsWith('.m4a') || 
                                 fileName.endsWith('.mp3') || fileName.endsWith('.aac')
      
      // å‹ç¼©æ ¼å¼ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼ï¼ˆ0.01ï¼‰ï¼ŒWAV ç­‰æœªå‹ç¼©æ ¼å¼ä½¿ç”¨ç¨é«˜çš„é˜ˆå€¼ï¼ˆ0.02ï¼‰
      const threshold = isCompressedFormat ? 0.01 : 0.02
      
      const isSilent = await isAudioSilentSimple(audioFile, threshold)
      
      if (isSilent) {
        console.log('ğŸ”‡ æ£€æµ‹åˆ°é™éŸ³éŸ³é¢‘ï¼Œè·³è¿‡ Whisper API è°ƒç”¨', {
          fileType,
          fileName: audioFile.name,
          threshold,
          isCompressedFormat
        })
        return NextResponse.json({ 
          transcripts: [],
          language: 'auto',
          message: 'no recording detected'
        })
      }
    } catch (silenceDetectionError: any) {
      // å¦‚æœé™éŸ³æ£€æµ‹å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†ï¼ˆä¸é˜»æ­¢è½¬å½•ï¼‰
      console.warn('âš ï¸ é™éŸ³æ£€æµ‹å¤±è´¥ï¼Œç»§ç»­å¤„ç†éŸ³é¢‘:', silenceDetectionError.message)
    }

    let transcriptionResult: { text: string; language?: string; segments?: any[] }

    try {
      // åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œä½¿ç”¨ LemonFox.ai çš„ baseURL
      const openai = new OpenAI({
        apiKey: process.env.WHISPER_API_KEY,
        baseURL: 'https://api.lemonfox.ai/v1',
      })

      // å‡†å¤‡éŸ³é¢‘æ–‡ä»¶
      // ç¡®å®šæ–‡ä»¶æ‰©å±•å
      let fileNameForApi = audioFile.name || 'audio.wav'
      if (!fileNameForApi.includes('.')) {
        const extension = audioFile.type?.split('/')[1] || 'wav'
        fileNameForApi = `audio.${extension}`
      }
      
      // è§£æè¯­è¨€å‚æ•°
      const { language: whisperLang } = parseLanguageForWhisper(language as any)
      
      // æ„å»ºè½¬å½•å‚æ•°
      const transcriptionParams: any = {
        file: audioFile,
        model: 'whisper-1',
        response_format: 'verbose_json', // ä½¿ç”¨ verbose_json æ ¼å¼ä»¥è·å–å¸¦æ—¶é—´æˆ³çš„ segments
      }

      // å¦‚æœ language ä¸æ˜¯ 'auto' æˆ– 'auto-translate'ï¼Œåˆ™ä¼ é€’ language å‚æ•°
      if (whisperLang && whisperLang !== 'auto' && whisperLang !== 'auto-translate') {
        transcriptionParams.language = whisperLang
      }

      // è°ƒç”¨ LemonFox.ai Whisper API
      const result = await openai.audio.transcriptions.create(transcriptionParams)
      
      console.log('âœ… LemonFox.ai Whisper API å“åº”æˆåŠŸ:', {
        hasText: !!result.text,
        textLength: result.text?.length || 0,
        hasSegments: !!(result as any).segments,
        segmentsCount: (result as any).segments?.length || 0,
        language: (result as any).language || 'unknown'
      })
      
      // LemonFox.ai API è¿”å›çš„æ ¼å¼ï¼ˆä¸ OpenAI å…¼å®¹ï¼‰ï¼š
      // {
      //   text: "...",
      //   language: "zh",
      //   duration: 123.45,
      //   segments: [
      //     { id: 0, start: 0.0, end: 5.0, text: "...", ... }
      //   ]
      // }
      transcriptionResult = {
        text: result.text,
        language: (result as any).language,
        segments: (result as any).segments || []
      }

      if (!transcriptionResult.text || transcriptionResult.text.trim().length === 0) {
        console.error('Transcription result has no text:', {
          hasText: !!transcriptionResult.text,
          textLength: transcriptionResult.text?.length || 0,
          segments: transcriptionResult.segments?.length || 0,
          result: transcriptionResult
        })
        throw new Error('No transcription text received from Whisper service')
      }

      // è§£æè½¬å½•ç»“æœ
      let transcriptText = transcriptionResult.text?.trim() || ''
      const segments = transcriptionResult.segments || [] // Whisper è¿”å›çš„ segmentsï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
      
      // æ£€æŸ¥è½¬å½•ç»“æœæ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œå¦‚æœæ˜¯åˆ™è½¬æ¢ä¸ºç®€ä½“
      // æˆ–è€…å¦‚æœç”¨æˆ·é€‰æ‹©äº†ä¸­æ–‡ç›¸å…³çš„è¯­è¨€é€‰é¡¹
      const detectedLanguage = transcriptionResult.language || ''
      const isChinese = language === 'zh' || 
                       language === 'auto' || 
                       language === 'auto-translate' ||
                       detectedLanguage === 'zh' ||
                       detectedLanguage === 'chinese' ||
                       /[\u4e00-\u9fa5]/.test(transcriptText)
      
      if (isChinese && transcriptText && !transcriptText.includes('(speaking in foreign language)')) {
        transcriptText = convertToSimplified(transcriptText)
      }

      // ä¿å­˜è½¬å½•åˆ°æ•°æ®åº“
      const { supabaseServiceRole } = await createClients()
      const transcripts = []

      // å¦‚æœæœ‰ segmentsï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰ï¼Œä½¿ç”¨ segmentsï¼›å¦åˆ™å›é€€åˆ°æŒ‰å¥å­åˆ†å‰²
      if (segments && segments.length > 0) {
        // ä½¿ç”¨ Whisper è¿”å›çš„ segmentsï¼Œæ¯ä¸ª segment éƒ½æœ‰å‡†ç¡®çš„æ—¶é—´æˆ³
        for (const segment of segments) {
          let segmentText = segment.text?.trim() || ''
          
          // è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
          if (isChinese && segmentText && !segmentText.includes('(speaking in foreign language)')) {
            segmentText = convertToSimplified(segmentText)
          }
          
          if (!segmentText) continue
          
          // ä½¿ç”¨ segment çš„æ—¶é—´æˆ³ï¼ˆç›¸å¯¹äºæ•´ä¸ªéŸ³é¢‘çš„å¼€å§‹æ—¶é—´ï¼Œå³ 0ï¼‰
          const segmentStartTime = segment.start || 0
          const segmentEndTime = segment.end || segment.start || 0
          const timestamp = formatTimestamp(segmentStartTime)

          const { data, error } = await supabaseServiceRole
            .from('transcripts')
            .insert({
              meeting_id: id,
              transcript: segmentText,
              timestamp: timestamp,
              audio_start_time: segmentStartTime,
              audio_end_time: segmentEndTime,
              confidence: 0.95
            })
            .select()
            .single()

          if (!error && data) {
            transcripts.push(data)
          }
        }
      } else {
        // å›é€€æ–¹æ¡ˆï¼šå¦‚æœæ²¡æœ‰ segmentsï¼ŒæŒ‰å¥å­åˆ†å‰²å¹¶ä¼°ç®—æ—¶é—´
        const sentences = transcriptText
          .split(/[.!?ã€‚ï¼ï¼Ÿ]\s*/)
          .map((s: string) => s.trim())
          .filter((s: string) => s.length > 0)

        if (sentences.length === 0) {
          sentences.push(transcriptText)
        }

        // è®¡ç®—æ—¶é—´æˆ³
        // å¦‚æœæœ‰å®é™…å½•éŸ³æ—¶é•¿ï¼Œä½¿ç”¨å®é™…æ—¶é•¿ï¼›å¦åˆ™ä¼°ç®—
        const totalDuration = audioDuration || (sentences.length * 4) // å¦‚æœæ²¡æœ‰æä¾›ï¼Œä¼°ç®—æ¯å¥4ç§’
        const timePerSentence = totalDuration / Math.max(1, sentences.length)

        for (let i = 0; i < sentences.length; i++) {
          const startTime = i * timePerSentence
          const endTime = (i + 1) * timePerSentence
          const timestamp = formatTimestamp(startTime)

          const { data, error } = await supabaseServiceRole
            .from('transcripts')
            .insert({
              meeting_id: id,
              transcript: sentences[i],
              timestamp: timestamp,
              audio_start_time: startTime,
              audio_end_time: endTime,
              confidence: 0.95
            })
            .select()
            .single()

          if (!error && data) {
            transcripts.push(data)
          }
        }
      }

      return NextResponse.json({ 
        transcripts,
        language: transcriptionResult.language || 'auto'
      })
    } catch (whisperError: any) {
      console.error('âŒ LemonFox.ai Whisper API é”™è¯¯:', whisperError)
      console.error('é”™è¯¯è¯¦æƒ…:', {
        message: whisperError.message,
        stack: whisperError.stack,
        name: whisperError.name
      })
      
      // å¦‚æœè½¬å½• API å¤±è´¥ï¼Œè¿”å›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
      return NextResponse.json({ 
        error: whisperError.message || 'Transcription service unavailable',
        details: process.env.NODE_ENV === 'development' 
          ? whisperError.stack 
          : 'è¯·æ£€æŸ¥ API é…ç½®å’Œç½‘ç»œè¿æ¥',
        transcripts: []
      }, { status: 500 })
    }
  } catch (error: any) {
    console.error('Error in POST /api/meetings/[id]/transcribe:', error)
    return NextResponse.json({ error: error.message }, { status: 500 })
  }
}
