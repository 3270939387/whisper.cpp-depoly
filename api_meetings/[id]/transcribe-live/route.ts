import { NextRequest, NextResponse } from 'next/server'
import { createClients } from '@/utils/supabase/server'
import { parseLanguageForWhisper } from '@/utils/transcription-language'
import { convertToSimplified } from '@/utils/chinese-converter'
// å®æ—¶è½¬å½•ä¸ä½¿ç”¨é™éŸ³æ£€æµ‹ï¼ˆç‰‡æ®µå¤ªçŸ­ï¼ŒRMSè®¡ç®—ä¸å‡†ç¡®ï¼‰
// import { isAudioSilentSimple } from '@/utils/audio-silence-detector'
import OpenAI from 'openai'

// Vercel å‡½æ•°é…ç½®ï¼šè®¾ç½®æœ€å¤§æ‰§è¡Œæ—¶é—´ä¸º 5 åˆ†é’Ÿï¼ˆ300ç§’ï¼‰
// å®æ—¶è½¬å½•ç‰‡æ®µé€šå¸¸è¾ƒçŸ­ï¼Œä½†éœ€è¦è¶³å¤Ÿçš„å¤„ç†æ—¶é—´
export const maxDuration = 300 // 5åˆ†é’Ÿ = 300ç§’
export const dynamic = 'force-dynamic'

// æ ¼å¼åŒ–æ—¶é—´æˆ³
function formatTimestamp(seconds: number): string {
  const mins = Math.floor(seconds / 60)
  const secs = Math.floor(seconds % 60)
  return `[${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}]`
}

// POST /api/meetings/[id]/transcribe-live - å®æ—¶è½¬å½•éŸ³é¢‘ç‰‡æ®µ
export async function POST(
  request: NextRequest,
  { params }: { params: Promise<{ id: string }> }
) {
  try {
    const { id } = await params
    const formData = await request.formData()
    const audioFile = formData.get('audio') as File
    const startTimeStr = formData.get('startTime') as string // éŸ³é¢‘ç‰‡æ®µçš„èµ·å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    const windowStartTimeStr = formData.get('windowStartTime') as string // çª—å£å¼€å§‹æ—¶é—´ï¼ˆç”¨äºé‡å è¯†åˆ«ï¼‰
    const lastProcessedTimeStr = formData.get('lastProcessedTime') as string // ä¸Šæ¬¡å¤„ç†çš„æ—¶é—´ç‚¹ï¼ˆç”¨äºå¢é‡è¯†åˆ«ï¼‰
    const language = formData.get('language') as string || 'auto' // è¯­è¨€å‚æ•°
    
    if (!audioFile) {
      return NextResponse.json({ error: 'No audio file provided' }, { status: 400 })
    }

    if (!startTimeStr) {
      return NextResponse.json({ error: 'No startTime provided' }, { status: 400 })
    }

    const startTime = parseFloat(startTimeStr)
    const windowStartTime = windowStartTimeStr ? parseFloat(windowStartTimeStr) : startTime
    const lastProcessedTime = lastProcessedTimeStr ? parseFloat(lastProcessedTimeStr) : 0

    // æ£€æŸ¥ API å¯†é’¥
    if (!process.env.WHISPER_API_KEY) {
      return NextResponse.json(
        { error: 'WHISPER_API_KEY not configured' },
        { status: 500 }
      )
    }

    console.log('ğŸ”Š ä½¿ç”¨ LemonFox.ai Whisper API è¿›è¡Œå®æ—¶è½¬å½•:', {
      audioFileSize: audioFile.size,
      audioFileType: audioFile.type,
      language: language,
      startTime: startTime
    })

    // å®æ—¶è½¬å½•è·³è¿‡é™éŸ³æ£€æµ‹ï¼š
    // 1. å®æ—¶ç‰‡æ®µè¾ƒçŸ­ï¼ˆ6ç§’ï¼‰ï¼ŒRMSè®¡ç®—å¯èƒ½ä¸å¤Ÿå‡†ç¡®ï¼Œå®¹æ˜“è¯¯åˆ¤
    // 2. å³ä½¿æœ‰é™éŸ³ï¼ŒWhisper APIä¹Ÿä¼šè¿”å›ç©ºç»“æœï¼Œä¸ä¼šé€ æˆé—®é¢˜
    // 3. è·³è¿‡é™éŸ³æ£€æµ‹å¯ä»¥å‡å°‘å»¶è¿Ÿï¼Œæé«˜å“åº”é€Ÿåº¦
    // 4. æœ€ç»ˆè½¬å½•ï¼ˆå®Œæ•´éŸ³é¢‘ï¼‰ä»ä¼šè¿›è¡Œé™éŸ³æ£€æµ‹
    // 
    // å¦‚æœéœ€è¦é™éŸ³æ£€æµ‹ï¼Œå¯ä»¥åœ¨æœ€ç»ˆè½¬å½•æ—¶è¿›è¡Œ

    let transcriptionResult: { text: string; language?: string; segments?: any[] }

    try {
      // åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œä½¿ç”¨ LemonFox.ai çš„ baseURL
      const openai = new OpenAI({
        apiKey: process.env.WHISPER_API_KEY,
        baseURL: 'https://api.lemonfox.ai/v1',
      })

      // å‡†å¤‡éŸ³é¢‘æ–‡ä»¶
      // ç¡®å®šæ–‡ä»¶æ‰©å±•å
      let fileName = audioFile.name || 'audio.webm'
      if (!fileName.includes('.')) {
        const extension = audioFile.type?.split('/')[1] || 'webm'
        fileName = `audio.${extension}`
      }
      
      // è§£æè¯­è¨€å‚æ•°
      const { language: whisperLang } = parseLanguageForWhisper(language as any)
      
      // æ„å»ºè½¬å½•å‚æ•°
      const transcriptionParams: any = {
        file: audioFile,
        model: 'whisper-1',
        response_format: 'verbose_json', // ä½¿ç”¨ verbose_json æ ¼å¼ä»¥è·å–å¸¦æ—¶é—´æˆ³çš„ segments
      }

      // å¦‚æœ language ä¸æ˜¯ 'auto' æˆ– 'auto-translate'ï¼Œåˆ™ä¼ é€’ language å‚æ•°
      if (whisperLang && whisperLang !== 'auto' && whisperLang !== 'auto-translate') {
        transcriptionParams.language = whisperLang
      }

      // è°ƒç”¨ LemonFox.ai Whisper API
      const result = await openai.audio.transcriptions.create(transcriptionParams)
      
      console.log('âœ… LemonFox.ai Whisper API å“åº”æˆåŠŸ:', {
        hasText: !!result.text,
        textLength: result.text?.length || 0,
        hasSegments: !!(result as any).segments,
        segmentsCount: (result as any).segments?.length || 0,
        language: (result as any).language || 'unknown'
      })
      
      // è½¬æ¢ç»“æœæ ¼å¼
      transcriptionResult = {
        text: result.text,
        language: (result as any).language,
        segments: (result as any).segments || []
      }

      if (!transcriptionResult.text || !transcriptionResult.text.trim()) {
        // å¦‚æœæ²¡æœ‰è½¬å½•æ–‡æœ¬ï¼Œè¿”å›ç©ºç»“æœ
        return NextResponse.json({ 
          transcripts: [],
          language: transcriptionResult.language || 'auto'
        })
      }

      // è§£æè½¬å½•ç»“æœ
      let transcriptText = transcriptionResult.text?.trim() || ''
      const segments = transcriptionResult.segments || [] // Whisper è¿”å›çš„ segmentsï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
      
      // æ£€æŸ¥è½¬å½•ç»“æœæ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œå¦‚æœæ˜¯åˆ™è½¬æ¢ä¸ºç®€ä½“
      // æˆ–è€…å¦‚æœç”¨æˆ·é€‰æ‹©äº†ä¸­æ–‡ç›¸å…³çš„è¯­è¨€é€‰é¡¹
      const detectedLanguage = transcriptionResult.language || ''
      const isChinese = language === 'zh' || 
                       language === 'auto' || 
                       language === 'auto-translate' ||
                       detectedLanguage === 'zh' ||
                       detectedLanguage === 'chinese' ||
                       /[\u4e00-\u9fa5]/.test(transcriptText)
      
      if (isChinese && transcriptText && !transcriptText.includes('(speaking in foreign language)')) {
        transcriptText = convertToSimplified(transcriptText)
      }

      // ä¿å­˜è½¬å½•åˆ°æ•°æ®åº“
      const { supabaseServiceRole } = await createClients()
      const transcripts = []

      // å¦‚æœæœ‰ segmentsï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰ï¼Œä½¿ç”¨ segmentsï¼›å¦åˆ™å›é€€åˆ°æŒ‰å¥å­åˆ†å‰²
      if (segments && segments.length > 0) {
        // ä½¿ç”¨ Whisper è¿”å›çš„ segmentsï¼Œæ¯ä¸ª segment éƒ½æœ‰å‡†ç¡®çš„æ—¶é—´æˆ³
        for (const segment of segments) {
          let segmentText = segment.text?.trim() || ''
          
          // è½¬æ¢ä¸ºç®€ä½“ä¸­æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
          if (isChinese && segmentText && !segmentText.includes('(speaking in foreign language)')) {
            segmentText = convertToSimplified(segmentText)
          }
          
          if (!segmentText) continue
          
          // ä½¿ç”¨ segment çš„æ—¶é—´æˆ³ï¼ˆç›¸å¯¹äºçª—å£å¼€å§‹æ—¶é—´ï¼‰
          // segment.start å’Œ segment.end æ˜¯ç›¸å¯¹äºéŸ³é¢‘ç‰‡æ®µçš„æ—¶é—´ï¼ˆç§’ï¼‰
          const segmentStartTime = windowStartTime + (segment.start || 0)
          const segmentEndTime = windowStartTime + (segment.end || segment.start || 0)
          const timestamp = formatTimestamp(segmentStartTime)
          
          // å¢é‡è¯†åˆ«ï¼šåªå¤„ç†ä¸Šæ¬¡å¤„ç†æ—¶é—´ä¹‹åçš„æ–°è½¬å½•
          if (segmentStartTime <= lastProcessedTime) {
            continue
          }
          
          // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ—¶é—´æ®µçš„è½¬å½•ï¼ˆå»é‡ï¼‰
          const { data: existing } = await supabaseServiceRole
            .from('transcripts')
            .select('id')
            .eq('meeting_id', id)
            .gte('audio_start_time', segmentStartTime - 0.5) // å…è®¸0.5ç§’çš„æ—¶é—´å®¹å·®
            .lte('audio_start_time', segmentStartTime + 0.5)
            .limit(1)
            .single()

          if (existing) {
            // å¦‚æœå·²å­˜åœ¨ï¼Œè·³è¿‡
            continue
          }

          const { data, error } = await supabaseServiceRole
            .from('transcripts')
            .insert({
              meeting_id: id,
              transcript: segmentText,
              timestamp: timestamp,
              audio_start_time: segmentStartTime,
              audio_end_time: segmentEndTime,
              confidence: 0.95
            })
            .select()
            .single()

          if (!error && data) {
            transcripts.push(data)
          }
        }
      } else {
        // å›é€€æ–¹æ¡ˆï¼šå¦‚æœæ²¡æœ‰ segmentsï¼ŒæŒ‰å¥å­åˆ†å‰²å¹¶ä¼°ç®—æ—¶é—´
        const sentences = transcriptText
          .split(/[.!?ã€‚ï¼ï¼Ÿ]\s*/)
          .map((s: string) => s.trim())
          .filter((s: string) => s.length > 0)

        if (sentences.length === 0) {
          sentences.push(transcriptText)
        }

        // ä¼°ç®—çª—å£æŒç»­æ—¶é—´ï¼ˆçº¦8ç§’ï¼‰
        const windowDuration = 8
        const timePerSentence = sentences.length > 0 ? windowDuration / sentences.length : windowDuration

        for (let i = 0; i < sentences.length; i++) {
          const sentenceStartTime = windowStartTime + (i * timePerSentence)
          const sentenceEndTime = windowStartTime + ((i + 1) * timePerSentence)
          const timestamp = formatTimestamp(sentenceStartTime)

          // å¢é‡è¯†åˆ«ï¼šåªå¤„ç†ä¸Šæ¬¡å¤„ç†æ—¶é—´ä¹‹åçš„æ–°è½¬å½•
          if (sentenceStartTime <= lastProcessedTime) {
            continue
          }

          // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ—¶é—´æ®µçš„è½¬å½•ï¼ˆå»é‡ï¼‰
          const { data: existing } = await supabaseServiceRole
            .from('transcripts')
            .select('id')
            .eq('meeting_id', id)
            .gte('audio_start_time', sentenceStartTime - 0.5) // å…è®¸0.5ç§’çš„æ—¶é—´å®¹å·®
            .lte('audio_start_time', sentenceStartTime + 0.5)
            .limit(1)
            .single()

          if (existing) {
            // å¦‚æœå·²å­˜åœ¨ï¼Œè·³è¿‡
            continue
          }

          const { data, error } = await supabaseServiceRole
            .from('transcripts')
            .insert({
              meeting_id: id,
              transcript: sentences[i],
              timestamp: timestamp,
              audio_start_time: sentenceStartTime,
              audio_end_time: sentenceEndTime,
              confidence: 0.95
            })
            .select()
            .single()

          if (!error && data) {
            transcripts.push(data)
          }
        }
      }

      return NextResponse.json({ 
        transcripts,
        language: transcriptionResult.language || 'auto'
      })
    } catch (whisperError: any) {
      console.error('âŒ LemonFox.ai Whisper API é”™è¯¯:', whisperError)
      
      // å¦‚æœè½¬å½• API å¤±è´¥ï¼Œè¿”å›ç©ºè½¬å½•è€Œä¸æ˜¯é”™è¯¯ï¼ˆå®æ—¶è½¬å½•å…è®¸å¤±è´¥ï¼‰
      return NextResponse.json({ 
        transcripts: [],
        language: 'auto'
      })
    }
  } catch (error: any) {
    console.error('Error in POST /api/meetings/[id]/transcribe-live:', error)
    return NextResponse.json({ error: error.message }, { status: 500 })
  }
}
